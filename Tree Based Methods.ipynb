{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first import the constructed files of predictors and response variables from R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "group1_x = pd.read_csv(\"data/group1_lag3_x.csv\")\n",
    "group1_y = pd.read_csv(\"data/group1_lag3_y.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "group2_x = pd.read_csv(\"data/group2_lag3_x.csv\")\n",
    "group2_y = pd.read_csv(\"data/group2_lag3_y.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "group3_x = pd.read_csv(\"data/group3_cross_x.csv\")\n",
    "group3_y = pd.read_csv(\"data/group3_cross_y.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "group4_x = pd.read_csv(\"data/group4_lag3_x.csv\")\n",
    "group4_y = pd.read_csv(\"data/group4_lag3_y.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As both tree based methods are not affected by the scale of the variables, we do not need to standardize the data for either of the two methods shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the sklearn function `GradientBoostingRegressor` along with `Multi Output Regressor` to adapt gradient boosting trees to a multi-target regression setting. We first need to tune the hyperparameters for the model: learning rate, number of iterations (known as `n_estimators` in the sklearn model), maximum depth of each tree, and whether we perform early stopping of the training or not. If we decide to allow for early stopping, we need to decide the number of iterations that will be used to decide. The default loss function is the squared loss.\n",
    "\n",
    "Note that because of the lack of interpretability of Gradient Boosting Trees (and Random Forests later), it is very difficult to estimate why some combination of parameters performed better than the others during the tuning process. However, what we can strive to achieve is check the relative importance of features for the final models that we arrive upon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_boosting_tree(model, x, y, window_length):\n",
    "    n_windows = x.shape[0] - window_length\n",
    "    predictions = pd.DataFrame(0, index=range(n_windows), columns=y.columns[1:])\n",
    "    y_actual = y.iloc[window_length:, 1:].reset_index()\n",
    "    \n",
    "    for i in range(n_windows):\n",
    "        x_data = x.iloc[i:(i+window_length), 1:]\n",
    "        y_data = y.iloc[i:(i+window_length), 1:]\n",
    "\n",
    "        g_boost_multi = MultiOutputRegressor(model)\n",
    "        g_boost_multi.fit(x_data, y_data)\n",
    "\n",
    "        pred_x = x.iloc[(i+window_length):(i+window_length+1), 1:]\n",
    "        pred_y = g_boost_multi.predict(pred_x)\n",
    "        \n",
    "        predictions.loc[i] = pred_y[0]\n",
    "        \n",
    "    sq_err = (y_actual.iloc[:, 1:]-predictions)**2\n",
    "    feb_start = y.index[y[\"Date\"] == '2020-02-01'][0]\n",
    "    sq_err_feb_start = y_actual.index[y_actual[\"index\"] == 152][0]\n",
    "    \n",
    "    mse_pre = sq_err.iloc[0:sq_err_feb_start, :].mean(axis=0)\n",
    "    mse_post = sq_err.iloc[sq_err_feb_start:, :].mean(axis=0)\n",
    "    mse_overall = sq_err.mean(axis=0)\n",
    "    \n",
    "    mse_df = pd.concat([mse_pre, mse_post, mse_overall], axis=1)\n",
    "\n",
    "    return (predictions, mse_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first tune `learning_rate=0.1, 0.2, 0.5` and `n_estimators=50, 100, 200` in conjuction, as there is a trade-off between the two parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_gbt = [\n",
    "     GradientBoostingRegressor(loss='ls', learning_rate=0.01, n_estimators=50, max_depth=3, n_iter_no_change=None),\n",
    "     GradientBoostingRegressor(loss='ls', learning_rate=0.1, n_estimators=100, max_depth=3, n_iter_no_change=None),\n",
    "     GradientBoostingRegressor(loss='ls', learning_rate=0.1, n_estimators=200, max_depth=3, n_iter_no_change=None),\n",
    "\n",
    "     GradientBoostingRegressor(loss='ls', learning_rate=0.2, n_estimators=50, max_depth=3, n_iter_no_change=None),\n",
    "     GradientBoostingRegressor(loss='ls', learning_rate=0.2, n_estimators=100, max_depth=3, n_iter_no_change=None),\n",
    "     GradientBoostingRegressor(loss='ls', learning_rate=0.2, n_estimators=200, max_depth=3, n_iter_no_change=None),\n",
    "\n",
    "     GradientBoostingRegressor(loss='ls', learning_rate=0.5, n_estimators=50, max_depth=3, n_iter_no_change=None),\n",
    "     GradientBoostingRegressor(loss='ls', learning_rate=0.5, n_estimators=100, max_depth=3, n_iter_no_change=None),\n",
    "     GradientBoostingRegressor(loss='ls', learning_rate=0.5, n_estimators=200, max_depth=3, n_iter_no_change=None)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mse_store = pd.DataFrame()\n",
    "for model in models_gbt:\n",
    "    predictions = train_and_predict_boosting_tree(model, group4_x, group4_y, 90)\n",
    "    mse_store = pd.concat([mse_store, predictions], axis=1)\n",
    "    print(predictions[1].iloc[:, 2].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we try `max_depth=3, 5, 10` for the best model chosen above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_gbt = [\n",
    "    GradientBoostingRegressor(loss='ls', learning_rate=0.1, n_estimators=50, max_depth=3, n_iter_no_change=None),\n",
    "    GradientBoostingRegressor(loss='ls', learning_rate=0.1, n_estimators=50, max_depth=5, n_iter_no_change=None),\n",
    "    GradientBoostingRegressor(loss='ls', learning_rate=0.1, n_estimators=50, max_depth=10, n_iter_no_change=None)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mse_store = pd.DataFrame()\n",
    "for model in models_gbt:\n",
    "    predictions = train_and_predict_boosting_tree(model, group3_x, group3_y, 90)\n",
    "    mse_store = pd.concat([mse_store, predictions], axis=1)\n",
    "    print(predictions.iloc[:, 2].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we look at training a Random Forest over the given data. We have two hyperparamters to tune - maximum depth (`max_depth`) of each tree, and the number of trees (`n_estimators`). Note that the default criterion is to measure the quality of the split is the mean squared error, and the default setting is to take a bootstrap sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_random_forest(model, x, y, window_length):\n",
    "    n_windows = x.shape[0] - window_length\n",
    "    predictions = pd.DataFrame(0, index=range(n_windows), columns=y.columns[1:])\n",
    "    y_actual = y.iloc[window_length:, 1:].reset_index()\n",
    "    \n",
    "    for i in range(n_windows):\n",
    "        x_data = x.iloc[i:(i+window_length), 1:]\n",
    "        y_data = y.iloc[i:(i+window_length), 1:]\n",
    "\n",
    "        random_forest = model\n",
    "        random_forest.fit(x_data, y_data)\n",
    "\n",
    "        pred_x = x.iloc[(i+window_length):(i+window_length+1), 1:]\n",
    "        pred_y = random_forest.predict(pred_x)\n",
    "        \n",
    "        predictions.loc[i] = pred_y[0]\n",
    "        \n",
    "    sq_err = (y_actual.iloc[:, 1:]-predictions)**2\n",
    "    feb_start = y.index[y[\"Date\"] == '2020-02-01'][0]\n",
    "    sq_err_feb_start = y_actual.index[y_actual[\"index\"] == feb_start][0]\n",
    "    \n",
    "    mse_pre = sq_err.iloc[0:sq_err_feb_start, :].mean(axis=0)\n",
    "    mse_post = sq_err.iloc[sq_err_feb_start:, :].mean(axis=0)\n",
    "    mse_overall = sq_err.mean(axis=0)\n",
    "    \n",
    "    mse_df = pd.concat([mse_pre, mse_post, mse_overall], axis=1)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider the following options for each parameter - `n_estimators=10, 100 (default), 200` and `max_depth=None (default), 3, 5, 10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_rf = [\n",
    "     RandomForestRegressor(criterion='mse', bootstrap=True, n_estimators=10, max_depth=None),\n",
    "     RandomForestRegressor(criterion='mse', bootstrap=True, n_estimators=10, max_depth=3),\n",
    "     RandomForestRegressor(criterion='mse', bootstrap=True, n_estimators=10, max_depth=5),\n",
    "     RandomForestRegressor(criterion='mse', bootstrap=True, n_estimators=10, max_depth=10),\n",
    "    \n",
    "     RandomForestRegressor(criterion='mse', bootstrap=True, n_estimators=50, max_depth=None),\n",
    "     RandomForestRegressor(criterion='mse', bootstrap=True, n_estimators=50, max_depth=3),\n",
    "     RandomForestRegressor(criterion='mse', bootstrap=True, n_estimators=50, max_depth=5),\n",
    "     RandomForestRegressor(criterion='mse', bootstrap=True, n_estimators=50, max_depth=10),\n",
    "    \n",
    "     RandomForestRegressor(criterion='mse', bootstrap=True, n_estimators=100, max_depth=None),\n",
    "     RandomForestRegressor(criterion='mse', bootstrap=True, n_estimators=100, max_depth=3),\n",
    "     RandomForestRegressor(criterion='mse', bootstrap=True, n_estimators=100, max_depth=5),\n",
    "     RandomForestRegressor(criterion='mse', bootstrap=True, n_estimators=100, max_depth=10),\n",
    "    \n",
    "     RandomForestRegressor(criterion='mse', bootstrap=True, n_estimators=200, max_depth=None),\n",
    "     RandomForestRegressor(criterion='mse', bootstrap=True, n_estimators=200, max_depth=3),\n",
    "     RandomForestRegressor(criterion='mse', bootstrap=True, n_estimators=200, max_depth=5),\n",
    "     RandomForestRegressor(criterion='mse', bootstrap=True, n_estimators=200, max_depth=10)\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mse_store = pd.DataFrame()\n",
    "for model in models_rf:\n",
    "    predictions = train_and_predict_random_forest(model, group4_x, group4_y, 90)\n",
    "    mse_store = pd.concat([mse_store, predictions], axis=1)\n",
    "    print(predictions.iloc[:, 2].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate average feature importances to see what predictors are preferred. We look at pre- and post-Feb, as well as overall, to get an idea on how the structural break affects values. We plot barplots for all three cases as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradident Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_models_gbt = [GradientBoostingRegressor(loss='ls', learning_rate=0.2, n_estimators=100),\n",
    "                    GradientBoostingRegressor(loss='ls', learning_rate=0.2, n_estimators=50),\n",
    "                    GradientBoostingRegressor(loss='ls', learning_rate=0.1, n_estimators=100),\n",
    "                    GradientBoostingRegressor(loss='ls', learning_rate=0.1, n_estimators=50, max_depth=5)]\n",
    "x_list = [group1_x, group2_x, group3_x, group4_x]\n",
    "y_list = [group1_y, group2_y, group3_y, group4_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "window_length = 90\n",
    "k = 0\n",
    "\n",
    "final_model_gbt_multi = MultiOutputRegressor(final_models_gbt[k])\n",
    "x = x_list[k]\n",
    "y = y_list[k]\n",
    "n_windows = x.shape[0] - window_length\n",
    "n_predictors = y.shape[1]-1\n",
    "n_covariates = x.shape[1]-1\n",
    "\n",
    "feature_imp_gbt = [[] for j in range(n_predictors)]\n",
    "for i in range(n_windows):\n",
    "    x_data = x.iloc[i:(i+window_length), 1:]\n",
    "    y_data = y.iloc[i:(i+window_length), 1:]\n",
    "\n",
    "    final_model_gbt_multi.fit(x_data, y_data)\n",
    "    for j in range(n_predictors):\n",
    "        feature_imp_gbt[j].append(final_model_gbt_multi.estimators_[j].feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_feature_imp_gbt = np.mean(np.mean(feature_imp_gbt, axis=1), axis=0)\n",
    "# Normalising them to sum up to 1\n",
    "avg_feature_imp_gbt = avg_feature_imp_gbt/sum(avg_feature_imp_gbt)\n",
    "\n",
    "avg_feature_imp_gbt_pre = np.mean([np.mean(i[0:62], axis=0) for i in feature_imp_gbt], axis=0)\n",
    "if sum(avg_feature_imp_gbt_pre) != 0:\n",
    "    avg_feature_imp_gbt_pre = avg_feature_imp_gbt_pre/sum(avg_feature_imp_gbt_pre)\n",
    "\n",
    "avg_feature_imp_gbt_post = np.mean([np.mean(i[62:], axis=0) for i in feature_imp_gbt], axis=0)\n",
    "avg_feature_imp_gbt_post = avg_feature_imp_gbt_post/sum(avg_feature_imp_gbt_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort labels in the same order as avg_feature_imp_gbt for graph labels\n",
    "sorted_barplot_labels_gbt = [x for _, x in sorted(zip(avg_feature_imp_gbt, range(n_covariates)))]\n",
    "sorted_barplot_labels_gbt_pre = [x for _, x in sorted(zip(avg_feature_imp_gbt_pre, range(n_covariates)))]\n",
    "sorted_barplot_labels_gbt_post = [x for _, x in sorted(zip(avg_feature_imp_gbt_post, range(n_covariates)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.bar(np.arange(n_covariates), list(sorted(avg_feature_imp_gbt)))\n",
    "plt.xticks(np.arange(n_covariates), sorted_barplot_labels_gbt, rotation='vertical')\n",
    "plt.ylabel(\"Normalized Feature Importance\")\n",
    "plt.title(\"Gradient Boosting Trees: Overall\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(np.arange(n_covariates), list(sorted(avg_feature_imp_gbt_pre)))\n",
    "plt.xticks(np.arange(n_covariates), sorted_barplot_labels_gbt_pre, rotation='vertical')\n",
    "plt.ylabel(\"Normalized Feature Importance\")\n",
    "plt.title(\"Gradient Boosting Trees: Pre Feb\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.bar(np.arange(n_covariates), list(sorted(avg_feature_imp_gbt_post)))\n",
    "plt.xticks(np.arange(n_covariates), sorted_barplot_labels_gbt_post, rotation='vertical')\n",
    "plt.ylabel(\"Normalized Feature Importance\")\n",
    "plt.title(\"Gradient Boosting Trees: Post Feb\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_models_rf = [RandomForestRegressor(criterion='mse', bootstrap=True, n_estimators=10, max_depth=10),\n",
    "                   RandomForestRegressor(criterion='mse', bootstrap=True, n_estimators=200, max_depth=5),\n",
    "                   RandomForestRegressor(criterion='mse', bootstrap=True, n_estimators=100, max_depth=3),\n",
    "                   RandomForestRegressor(criterion='mse', bootstrap=True, n_estimators=100, max_depth=5)]\n",
    "x_list = [group1_x, group2_x, group3_x, group4_x]\n",
    "y_list = [group1_y, group2_y, group3_y, group4_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_length = 90\n",
    "k = 2\n",
    "\n",
    "final_model_rf = final_models_rf[k]\n",
    "x = x_list[k]\n",
    "y = y_list[k]\n",
    "n_windows = x.shape[0] - window_length\n",
    "n_predictors = y.shape[1]-1\n",
    "n_covariates = x.shape[1]-1\n",
    "\n",
    "feature_imp_rf = []\n",
    "for i in range(n_windows):\n",
    "    x_data = x.iloc[i:(i+window_length), 1:]\n",
    "    y_data = y.iloc[i:(i+window_length), 1:]\n",
    "\n",
    "    final_model_rf.fit(x_data, y_data)\n",
    "    feature_imp_rf.append(final_model_rf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_feature_imp_rf = np.mean(feature_imp_rf, axis=0)\n",
    "avg_feature_imp_rf = avg_feature_imp_rf/sum(avg_feature_imp_rf)\n",
    "\n",
    "avg_feature_imp_rf_pre = np.mean(feature_imp_rf[0:62], axis=0)\n",
    "if sum(avg_feature_imp_rf_pre) != 0:\n",
    "    avg_feature_imp_rf_pre = avg_feature_imp_rf_pre/sum(avg_feature_imp_rf_pre)\n",
    "\n",
    "avg_feature_imp_rf_post = np.mean(feature_imp_rf[62:], axis=0)\n",
    "avg_feature_imp_rf_post = avg_feature_imp_rf_post/sum(avg_feature_imp_rf_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_barplot_labels_rf = [x for _, x in sorted(zip(avg_feature_imp_rf, range(n_covariates)))]\n",
    "sorted_barplot_labels_rf_pre = [x for _, x in sorted(zip(avg_feature_imp_rf_pre, range(n_covariates)))]\n",
    "sorted_barplot_labels_rf_post = [x for _, x in sorted(zip(avg_feature_imp_rf_post, range(n_covariates)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.bar(np.arange(n_covariates), list(sorted(avg_feature_imp_rf)))\n",
    "plt.xticks(np.arange(n_covariates), sorted_barplot_labels_rf, rotation='vertical')\n",
    "plt.ylabel(\"Normalized Feature Importance\")\n",
    "plt.title(\"Random Forest: Overall\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(np.arange(n_covariates), list(sorted(avg_feature_imp_rf_pre)))\n",
    "plt.xticks(np.arange(n_covariates), sorted_barplot_labels_rf_pre, rotation='vertical')\n",
    "plt.ylabel(\"Normalized Feature Importance\")\n",
    "plt.title(\"Random Forest: Pre Feb\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.bar(np.arange(n_covariates), list(sorted(avg_feature_imp_rf_post)))\n",
    "plt.xticks(np.arange(n_covariates), sorted_barplot_labels_rf_post, rotation='vertical')\n",
    "plt.ylabel(\"Normalized Feature Importance\")\n",
    "plt.title(\"Random Forest: Post Feb\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
