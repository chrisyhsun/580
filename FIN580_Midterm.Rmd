---
title: 'FIN580: Midterm'
author: "Chris, Utsav, Srishti"
date: "Spring 2020"
output:
  html_document:
    df_print: paged
    toc: no
    toc_depth: 4
  word_document:
    toc: no
    toc_depth: '4'
  pdf_document:
    fig_caption: yes
    toc: no
    toc_depth: 4
geometry: margin=1.5in
editor_options:
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.align="center", fig.height=5.5, fig.width=6, collapse=TRUE, comment="", prompt=TRUE, echo=TRUE, cache=TRUE, autodep=TRUE, cache.comments=FALSE)
options(width=63)
```

--------

# Objective

To forecast daily volatilities of major stock indexes after the COVID-19 outbreak and provide risk measures than can help portfolio managers to make investment decisions during this turbulent period.

--------

# Data Preparation

I use the following libraries and set my working directory.
```{r warning=FALSE, error=FALSE}
library('fasttime')
library('data.table')
library('dplyr')
library('matrixStats')
library('bit64')
library('ggplot2')
library('gdata')
library('naniar')
library('mice')
library('forecast')
library('imputeTS')
library('zoo')
library('png')
library('grid')
library('HDeconometrics')
library('tictoc')
library('cluster')
library('factoextra')
library('tseries')
```

Our dataset has values for daily open, close and RV for 31 indices. 

```{r read_data}
data <- fread('data/data.csv')
summary(data)
```

```{r checking_time}
times <- substr(data$Date, 12, nchar(data$Date))
unique_times <- unique(times)
dates1 <- unique(substr(data[substr(Date, 12, 26) == unique_times[1]]$Date, 1, 10))
dates2 <- unique(substr(data[substr(Date, 12, 26) == unique_times[2]]$Date, 1, 10))
intersect(dates1, dates2) # returns character(0), so can drop times
```

```{r checking_unique_symbols}
symbols <- unlist(unique(data[, "Symbol"]))
for (sym in symbols) {
  subset_dates <- data["Symbol" == sym, "Date"]
  if (length(subset_dates) != length(unique(subset_dates))) {
    print(sym) 
  }
}
print("Done")
```


We first transform our data to show these values as columns (three for each index) such that each row corresponds to a unique day.

```{r transform_data}
#transforming data to get value for each security as a column
grid <- as.data.frame(as.character(unique(data$Date)))
names(grid) <- "Date"
symbols <- as.character(unique(data$Symbol))

for (i in 1:length(symbols)){
  mergewith <- data[as.character(data$Symbol)==symbols[i],-c(2)]
  names(mergewith) <- c("Date", paste0("open",symbols[i]), paste0("close",symbols[i]), paste0("rv",symbols[i]))
  grid <- merge(grid, mergewith, by=c("Date"), all = TRUE )
}
# grid has 5319 rows, and 94 columns

sample <- grid[2625:5303,]
# 2679 rows to get data starting from 2010 (we need to choose this carefully)

index_names <- substr(symbols, 2, nchar(symbols))
index_countries <- c("Amsterdam", "Australia", "Belgium", "India", "Portugal", "Brazil", "USA", "France", "Italy", "UK", "Germany", "Canada", "Hong Kong", "Spain", "USA", "Korea", "Pakistan", "Mexico", "Japan", "India2", "Denmark", "Finland", "Sweden", "Norway", "UK2", "Spain2", "USA", "China", "Switzerland", "Singapore", "EU")
```

```{r na_data_function, cache=TRUE}
count_col_na <- function(data) {
  sapply(data, function(x) sum(is.na(x)))  
}
```


```{r show_data_head}
head(sample[, 1:7])
```


Checking how much data is missing-
```{r}
col <- seq(from=2, to=92, by = 3)
sample2 <- sample[, col]
colnames(sample2) <- index_names
rownames(sample2) <- substr(sample$Date, 1, 10)
```

```{r}
index <- seq(2, nrow(sample), 500)
date_labels <- substr(sample$Date[index], 1, 10)

vis_miss(sample2, show_perc_col=FALSE) +
  theme(axis.text.x = element_text(angle=90)) +
  scale_y_continuous(breaks = index, labels=date_labels) +
  theme(axis.text.y = element_text(angle=45)) + 
  ylab("Date")
```

```{r}
col <- c(1, seq(from=2, to=92, by = 3))
vis_miss(sample[,col]) # total 6.7% missing data, most for BVLG and STI
gg_miss_var(sample[,col])
```

```{r fig.width=10, fig.height=7,echo=FALSE}
img <- readPNG("img/missingdata.png")
grid.newpage()
grid.raster(img)
```

```{r fig.width=5, fig.height=3.5,echo=FALSE}
img <- readPNG("img/missingcount.png")
grid.newpage()
grid.raster(img)
```

Also, we check what are the sizes of continuous gaps of data. The max continuous gaps as we already know are for BVLG and STI in the begining of the data. Apart from that the gap range is at max 5.

Dates where no data is missing
```{r}
ind <- which(rowSums(is.na(grid)) == 0)
grid$Date[ind]
```


## Different ways of dealing with missing data

**Linear interpolation**

```{r}
linear_imputed_data <- na_interpolation(sample, option = "linear") 
```

**Spline interpolation**

```{r}
spline_imputed_data <- na_interpolation(sample, option = "spline") 
```

```{r}
constant_imputed_data <- na.locf(sample, na.rm=FALSE)
```

To visualize what's happening-

Want to see what happens in case of an index like AORD

```{r}
plotNA.imputations(sample$open.AORD[1400:1800], linear_imputed_data$open.AORD[1400:1800], ylab='', xlab='Date', main='', xaxt='n')
axis(1, at=c(0, 1/5, 2/5, 3/5, 4/5, 1), labels=substr(sample$Date[c(1400, 1480, 1560, 1640, 1720, 1800)], 1, 7), las=0)
```

```{r fig.width=3.5, fig.height=3,echo=FALSE}
img <- readPNG("img/AORDlinear.png")
grid.newpage()
grid.raster(img)
```

```{r}
plotNA.imputations(sample$open.AORD[1400:1800], spline_imputed_data$open.AORD[1400:1800], ylab='', xlab='Date', main='', xaxt='n')
axis(1, at=c(0, 1/5, 2/5, 3/5, 4/5, 1), labels=substr(sample$Date[c(1400, 1480, 1560, 1640, 1720, 1800)], 1, 7), las=0)
```

```{r fig.width=3.5, fig.height=3,echo=FALSE}
img <- readPNG("img/AORDspline.png")
grid.newpage()
grid.raster(img)
```

Constant Imputation

```{r}
plotNA.imputations(sample$open.AORD[1400:1800], constant_imputed_data$open.AORD[1400:1800], ylab='', xlab='Date', main='', xaxt='n')
axis(1, at=c(0, 1/5, 2/5, 3/5, 4/5, 1), labels=substr(sample$Date[c(1400, 1480, 1560, 1640, 1720, 1800)], 1, 7), las=0)
```

```{r fig.width=3.5, fig.height=3,echo=FALSE}
img <- readPNG("img/AORDconstant.png")
grid.newpage()
grid.raster(img)
```

For STI which has a lot of missing data in the beginning itself what do the three models do-

(The linear method basically fills one value for them all whereas spline does not, constant doesn't fill in any data.)

```{r}
plotNA.imputations(sample$open.STI[1400:1800], linear_imputed_data$open.STI[1400:1800], ylab='', xlab='Date', main='', xaxt='n')
axis(1, at=c(0, 1/5, 2/5, 3/5, 4/5, 1), labels=substr(sample$Date[c(1400, 1480, 1560, 1640, 1720, 1800)], 1, 7), las=0)
```

```{r fig.width=3.5, fig.height=3,echo=FALSE}
img <- readPNG("img/STIlinear.png")
grid.raster(img)
```

```{r}
plotNA.imputations(sample$open.STI[1400:1800], spline_imputed_data$open.STI[1400:1800], ylab='', xlab='Date', main='', xaxt='n')
axis(1, at=c(0, 1/5, 2/5, 3/5, 4/5, 1), labels=substr(sample$Date[c(1400, 1480, 1560, 1640, 1720, 1800)], 1, 7), las=0)
```

```{r fig.width=3.5, fig.height=3,echo=FALSE}
img <- readPNG("img/STIspline.png")
grid.raster(img)
```

```{r}
plotNA.imputations(sample$open.STI[1400:1800], constant_imputed_data$open.STI[1400:1800], ylab='', xlab='Date', main='', xaxt='n')
axis(1, at=c(0, 1/5, 2/5, 3/5, 4/5, 1), labels=substr(sample$Date[c(1400, 1480, 1560, 1640, 1720, 1800)], 1, 7), las=0)
```

Linear interploation looks a like a good way of dealing with missing data. 

## Working on what variables to use for predicting RVs

As RVs are generally consistent over time, using lagged values as a variable would be a good idea. Here are the autocorrelation charts for each of the RV series-

```{r}
par(mfrow=c(4,4))
col <- seq(from=52, to=94, by = 3)
for (j in col){
   acf(linear_imputed_data[,j], main=paste0(names(linear_imputed_data)[j]))
}
```

```{r fig.width=7, fig.height=4,echo=FALSE}
img <- readPNG("img/acf1.png")
grid.newpage()
grid.raster(img)
```

```{r fig.width=7, fig.height=4,echo=FALSE}
img <- readPNG("img/acf2.png")
grid.newpage()
grid.raster(img)
```

We would also be interested in seeing how the cross rv terms relate. As the interactions would be large (31C2 i.e. 465 terms), one of the ways that we decided to look at is average correlation over time using 500 values in each window. (Let us remove BVLG, FTMIB, STI and remove 1152 intial rows (now data starts from roughly Oct 2005) as their std would be zero when a constant value is imputed)


```{r}
col <- seq(from=4, to=94, by = 3)
# rvs <- na_interpolation(grid[1512:5303,col], option = "linear") 
rvs <- na.locf(grid, na.rm=FALSE)[1512:5303, col]
rvs <- rvs[,-c(5,9,30)]
covars <- c()
window <- 500
for (i in 1:(nrow(rvs)-(window-1))){
  sub <- rvs[i:(window-1+i),]
  mean_cor <- mean(cor(sub))
  covars <- c(covars, mean_cor)
}
plot(as.Date(grid$Date[2011:5303]), covars, type='l', xlab="Years", ylab="Average Correlation", col='blue') # roughly starting from Sept, 2007
```

```{r fig.width=7, fig.height=5,echo=FALSE}
img <- readPNG("img/correlation.png")
grid.newpage()
grid.raster(img)
```

*90 day running correlation (linear)*

```{r}
col <- seq(from=4, to=94, by = 3)
rvs <- na_interpolation(grid[1512:5303,col], option = "linear") 
rvs <- rvs[,-c(5,9,30)]
covars <- c()
window <- 90
for (i in 1:(nrow(rvs)-(window-1))){
  sub <- rvs[i:(window-1+i),]
  mean_cor <- mean(cor(sub))
  covars <- c(covars, mean_cor)
}
plot(as.Date(grid$Date[1601:5303]), covars, type='l', xlab="Years", ylab="Average Correlation", main = "Rolling 90 days Correlation") # roughly starting from Sept, 2007
```


```{r}
col <- seq(from=4, to=94, by = 3)
rvs <- na.locf(grid[1512:5303,col], na.rm=FALSE) 
rvs <- rvs[,-c(5,9,30)]
covars <- c()
window <- 90
for (i in 1:(nrow(rvs)-(window-1))){
  sub <- rvs[i:(window-1+i),]
  mean_cor <- mean(cor(sub))
  covars <- c(covars, mean_cor)
}
plot(as.Date(grid$Date[1601:5303]), covars, type='l', xlab="Years", ylab="Average Correlation", main = "Rolling 90 days Correlation") # roughly starting from Sept, 2007
```

This variable could be used for switching between models as when market is in distress, correlations tend to peak and maybe a different model from usual could be suggested.

## Addtional datasets that we can use

Maybe like NVIX or Economic Uncertainity Data (It is however only mothly and last data point available is Jan 2020)

## Checking for normality of $\epsilon_t$

We calculate daily standardized returns and check for their normality using Shapiro-Wilk Normality Test. If p-value given by the test is greater than 0.05 then we can not reject the normality assumption.

```{r}
new_data <- fread('data/final_data.csv')
new_data <- new_data[, -c(1, 96)]
```

```{r}
#grid
col <- seq(from=2, to=92, by = 3)
pvalsw <- c()
pvaljb <- c()
for (x in col){
  sub <- new_data[,c(..x, ..x+1, ..x+2)]
  eps <- log(sub[,2]/sub[,1])/sqrt(sub[,3])
  eps <- unlist(eps)
  print(paste(x, which(is.na(eps))))
  eps <- eps[!is.na(eps)]
  # Check: 11 25
  # Check: "59 106" "59 131" "59 141" "59 187" "59 197" "59 207" "59 208"
  a <- shapiro.test(eps)
  b <- jarque.bera.test(eps)
  pvalsw <- c(pvalsw, a$p.value)
  pvaljb <- c(pvaljb, b$p.value)
  #qqnorm(eps)
  #abline(0,1)
  
  #h <- hist(eps, breaks = 10, density = 10,
          #col = "lightgray", xlab = "Accuracy", main = "Overall") 
  #xfit <- seq(min(eps), max(eps), length = 40) 
  #yfit <- dnorm(xfit, mean = mean(eps), sd = sd(eps)) 
  #yfit <- yfit * diff(h$mids[1:2]) * length(eps) 
  #lines(xfit, yfit, col = "black", lwd = 2)
}

```

At 5% significance level, we have to reject the null hypothesis (that the data is normal) for 24 out of 31 indices.

```{r}
plot(pvaljb, ylab='p-values', type='n', xaxt='n', xlab='')
text(pvaljb, index_names, cex=0.75)
abline(h=0.05, lty=2, col='red')
```

```{r}
names(pvalsw) <- index_names
barplot(pvalsw, ylim=c(0, 1), ylab='p-values', xlab='', las=2, cex.names=0.9)
# text(pvalsw, index_names, cex=0.75)
abline(h=0.05, lty=2, col='red')
```

```{r fig.width=5, fig.height=4,echo=FALSE}
img <- readPNG("img/pvalue.png")
grid.newpage()
grid.raster(img)
```

```{r}
plot(pvaljb, main="P-values for different Indices - JB", ylab='p value')
abline(h=0.05, lty=2, col='red')
```

Plotting both p-values together
```{r}
plot(pvalsw, pvaljb, ylim=c(0, 1), xlim=c(0,1), xlab="p-values (Shapiro-Wilk)", ylab="p-values (Jarque-Bera)")
# text(pvalsw, pvaljb, index_names, cex=0.75)
abline(h=0.05, lty=2, col='red')
abline(v=0.05, lty=2, col='red')
```

So the two tests agree on all indices but one:
```{r}
sw_reject <- which(pvalsw < 0.05)
jb_reject <- which(pvaljb < 0.05)
print(sw_reject)
print(jb_reject)
print(intersect(sw_reject, jb_reject))
print(index_names[intersect(sw_reject, jb_reject)])
```

# Building Baseline models

We want to start the forecasts from February 1st, 2020.

a) Random Walk: For predicting value at time t, use the value at time t-1.

```{r}
finaldata <- fread("data/final_data.csv")
col <- c(2,seq(from=5, to=95, by = 3))
finaldata <- finaldata[, ..col]


feb_start <- which(finaldata$Date == '2020-02-01')
sq_errs <- diff(as.matrix(finaldata[, c(-1)]))^2
sq_errs_pre <- sq_errs[3:(feb_start-2),]
sq_errs_post <- sq_errs[(feb_start-1):nrow(sq_errs),]
mse_pre <- colMeans(sq_errs_pre)
mse_post <- colMeans(sq_errs_post)
mse_overall <- colMeans(sq_errs[3:nrow(sq_errs),])

mse_csv <- cbind(index_names, mse_pre, mse_post, mse_overall)
write.csv(mse_csv, "mse_rw.csv", row.names = FALSE )
```

b) Heterogeneous Autoregressive (HAR) model: We can use 200 days (to account for recency) rolling window for our predictions?

```{r}
col <- c(1,seq(from=4, to=94, by = 3))
traindata <- finaldata[,col]
mean5 <- as.data.frame(rollmean(zoo(traindata[, -c(1)]), 5))
mean22 <- as.data.frame(rollmean(zoo(traindata[, -c(1)]), 22))

testdata <- finaldata[year(finaldata$Date)==2020,col]
rv_actual <- testdata[month(testdata$Date)>=2, ]
n <- nrow(rv_actual)

# for 90 day rolling windows
window <- 90
rv <- tail(traindata, (window+n+1))
rv5 <- tail(mean5, (window+n+1))
rv22 <- tail(mean22, (window+n+1))

#par(mfrow=c(3,3))
mse_har <- c()
for (s in 2:32){
  err <- c()
  for (x in 1:n){
    tab <- as.data.frame(cbind(rv[x:(window - 1 +x),s], rv5[x:(window - 1+x),(s-1)], rv22[x:(window - 1 +x),(s-1)], rv[(x+1):(window+x),s]))
    linearMod <- lm(V4 ~ ., data=tab) 
    x_new <- as.data.frame(cbind(rv[(window+x),s], rv5[(window+x),(s-1)], rv22[(window+x),(s-1)]))
    rv_pred <- predict(linearMod, x_new)
    rv_actual_val <- rv_actual[x,s]
    err <- c(err, (rv_pred - rv_actual_val)^2)
  }
  #plot(err)
  mse_har <- c(mse_har, mean(err))
}
plot(mse_har, main="Mse for each index with HAR Predictions")
```

```{r fig.width=5, fig.height=4,echo=FALSE}
img <- readPNG("img/msehar.png")
grid.newpage()
grid.raster(img)
```

Comparing HAR mse (black) with Random Walk MSE (red) - in this case Random Walk does better.

```{r fig.width=5, fig.height=4,echo=FALSE}
img <- readPNG("img/mse2.png")
grid.newpage()
grid.raster(img)
```

```{r}
realized_covariances <- fread('data/final_data.csv')
dates <- realized_covariances[, c("Date")]
realized_covariances <- realized_covariances[, -c("V1", "Date", "covars")]
realized_covariances <- realized_covariances[, c(seq(3, 93, 3))]
```

```{r}
group1 <- c(3, 4, 5, 8, 9, 20, 21, 22, 23, 24, 26)
group2 <- c(1, 2, 6, 7, 11, 12, 15, 25, 27, 29, 31)
group3 <- c(18, 28)
group4 <- c(10, 13, 14, 16, 17, 19, 30)
```

first lag and second order terms

```{r}
group_ind <- group3
n_rows <- nrow(realized_covariances)

# First lag all
x <- realized_covariances[1:(nrow(realized_covariances)-1), ]
y <- realized_covariances[2:nrow(realized_covariances), ..group_ind]
y_date <- dates[2:nrow(realized_covariances)]

second_order_terms <- c()
for (i in 1:nrow(x)) {
  row <- unlist(x[i, ..group_ind])
  outer_product <- outer(row, row)
  flatten <- outer_product[lower.tri(outer_product, diag=TRUE)]
  second_order_terms <- rbind(second_order_terms, flatten)
}

x <- cbind(x, second_order_terms)

x <- cbind(y_date, x)
y <- cbind(y_date, y)

print(dim(x))
print(dim(y))
```

Lags 
```{r}
group_ind <- group2
lag <- 3

n_rows <- nrow(realized_covariances) - lag

# First lag all
x <- realized_covariances[lag:(nrow(realized_covariances)-1), ]
y <- realized_covariances[(lag+1):nrow(realized_covariances), ..group_ind]
y_date <- dates[(lag+1):nrow(realized_covariances)]


if (lag > 1) {
  for (i in 2:lag) {
    print(dim(x))
    x <- cbind(x, realized_covariances[(lag+1-i):(nrow(realized_covariances)-i), ..group_ind])   
  }  
}

x <- cbind(y_date, x)
y <- cbind(y_date, y)

print(dim(x))
print(dim(y))
```


```{r}
window_length <- 90
n_windows <- nrow(x) - window_length

predictions <- matrix(0, nrow=n_windows, ncol=length(group_ind))
coef_arr <- c()

# For each realized covariance, train a LASSO model
for (i in 1:length(group_ind)) {
  coefs <- c()
  for (j in 1:n_windows) {
    x_data <- x[j:(j+window_length-1),]
    y_data <- unlist(y[j:(j+window_length-1), ..i])
    
    lasso <- ic.glmnet(x_data, y_data, crit="aic")
    
    tau=1
    first.step.coef=coef(lasso)[-1]
    penalty.factor=(abs(first.step.coef)+1/sqrt(nrow(x_data)))^(-tau)
    adalasso=ic.glmnet(x_data, y_data, crit="aic", penalty.factor=penalty.factor)
    # size of coefs: no. of covariates * no. of windows
    # Stores the lasso coefficients for every windows
    coefs <- cbind(coefs, coef(adalasso))
    
    next_min_data <- x[j+window_length, ]
    predictions[j, i] <- predict(adalasso, next_min_data) 
  }
  # coef_arr stores coefs for each of the 31 dependent variables
  coef_arr[[i]] <- coefs
}

# Calculate the proportion of beta assigned to each predictor
coef_arr_norm <- c()
for (i in 1:length(group_ind)) {
  coefs <- coef_arr[[i]]
  # normalized_coefs is also of size no. of covariates * no. of windows
  # except that every column has been normalized to add up to 1
  normalized_coefs <- abs(coefs) %*% diag(1/colSums(abs(coefs)))
  # Take average across all windows to get vector of length no. of covariates
  # representing average proportion of beta on each covariate.
  coef_arr_norm[[i]] <- rowMeans(normalized_coefs)
}

coef_mat <- matrix(0, nrow=(length(coef_arr_norm[[1]])), ncol=length(group_ind))
for (i in 1:length(group_ind)) {
  coef_mat[, i] <- coef_arr_norm[[i]]
}

actual_data <- y[(1+window_length):nrow(y),]
col_mse <- colMeans((actual_data - predictions)**2)
overall_mse <- mean(col_mse)
print(col_mse)
print(overall_mse)
```

```{r}
rw_pred <- y[window_length:(nrow(y)-1),]
rw_col_mse <- colMeans((actual_data - rw_pred)**2)
rw_overall_mse <- mean(rw_col_mse)
print(rw_col_mse)
print(rw_overall_mse)
```

```{r}
rw_pred <- y[window_length:(nrow(y)-1),]*-0.95
rw_col_mse <- colMeans((actual_data - rw_pred)**2)
rw_overall_mse <- mean(rw_col_mse)
print(rw_col_mse)
print(rw_overall_mse)
```