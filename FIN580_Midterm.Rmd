---
title: 'FIN580: Midterm'
author: "Chris, Utsav, Srishti"
date: "Spring 2020"
output:
  html_document:
    df_print: paged
    toc: no
    toc_depth: 4
  word_document:
    toc: no
    toc_depth: '4'
  pdf_document:
    fig_caption: yes
    toc: no
    toc_depth: 4
geometry: margin=1.5in
editor_options:
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.align="center", fig.height=5.5, fig.width=6, collapse=TRUE, comment="", prompt=TRUE, echo=TRUE, cache=TRUE, autodep=TRUE, cache.comments=FALSE)
options(width=63)
```

--------

# Objective

To forecast daily volatilities of major stock indexes after the COVID-19 outbreak and provide risk measures than can help portfolio managers to make investment decisions during this turbulent period.

--------

# Data Preparation

I use the following libraries and set my working directory.
```{r warning=FALSE, error=FALSE}
library('fasttime')
library('data.table')
library('dplyr')
library('matrixStats')
library('bit64')
library('ggplot2')
library('gdata')
library('naniar')
library('mice')
library('forecast')
library('imputeTS')
library('zoo')
library('png')
library('grid')
```

Our dataset has values for daily open, close and RV for 31 indices. 

```{r read_data}
data <- fread('data.csv')
summary(data)
```

We first transform our data to show these values as columns (three for each index) such that each row corresponds to a unique day.

```{r transform_data}
#transforming data to get value for each security as a column
grid <- as.data.frame(as.character(unique(data$Date)))
names(grid) <- "Date"
symbols <- as.character(unique(data$Symbol))

for (i in 1:length(symbols)){
  mergewith <- data[as.character(data$Symbol)==symbols[i],-c(2)]
  names(mergewith) <- c("Date", paste0("open",symbols[i]), paste0("close",symbols[i]), paste0("rv",symbols[i]))
  grid <- merge(grid, mergewith, by=c("Date"), all = TRUE )
}
# grid has 5303 rows, and 94 columns

sample <- grid[2625:5303,]
# 2679 rows to get data starting from 2010 (we need to choose this carefully)

open_col_names <- colnames(grid[, seq(from=2, to=92, by = 3)])
index_names <- substr(open_col_names, 6, nchar(open_col_names))
```

```{r na_data_function, cache=TRUE}
count_col_na <- function(data) {
  sapply(data, function(x) sum(is.na(x)))  
}
```


```{r show_data_head}
head(sample[, 1:7])
```

Checking how much data is missing-

```{r}
col <- c(1, seq(from=2, to=92, by = 3))
vis_miss(sample[,col]) # total 6.7% missing data, most for BVLG and STI
gg_miss_var(sample[,col])
```

```{r fig.width=10, fig.height=7,echo=FALSE}
img <- readPNG("img/missingdata.png")
grid.newpage()
grid.raster(img)
```

```{r fig.width=5, fig.height=3.5,echo=FALSE}
img <- readPNG("img/missingcount.png")
grid.newpage()
grid.raster(img)
```

Also, we check what are the sizes of continuous gaps of data. The max continuous gaps as we already know are for BVLG and STI in the begining of the data. Apart from that the gap range is at max 5.

## Different ways of dealing with missing data

**Linear interpolation**

```{r}
linear_imputed_data <- na_interpolation(sample, option = "linear") 
```

**Spline interpolation**

```{r}
spline_imputed_data <- na_interpolation(sample, option = "spline") 
```

```{r}
constant_imputed_data <- na.locf(sample, na.rm=FALSE)
```

To visualize what's happening-

Want to see what happens in case of an index like AORD

```{r}
plotNA.imputations(sample$open.AORD[1:300], linear_imputed_data$open.AORD[1:300])
```

```{r fig.width=3.5, fig.height=3,echo=FALSE}
img <- readPNG("img/AORDlinear.png")
grid.newpage()
grid.raster(img)
```

```{r}
plotNA.imputations(sample$open.AORD[1:300], spline_imputed_data$open.AORD[1:300])
```

```{r fig.width=3.5, fig.height=3,echo=FALSE}
img <- readPNG("img/AORDspline.png")
grid.newpage()
grid.raster(img)
```

Constant Imputation

```{r}
plotNA.imputations(sample$open.AORD[1:300], constant_imputed_data$open.AORD[1:300])
```

```{r fig.width=3.5, fig.height=3,echo=FALSE}
img <- readPNG("img/AORDconstant.png")
grid.newpage()
grid.raster(img)
```

For STI which has a lot of missing data in the beginning itself what do the three models do-

(The linear method basically fills one value for them all whereas spline does not, constant doesn't fill in any data.)

```{r}
plotNA.imputations(sample$open.STI[1000:1800], linear_imputed_data$open.STI[1000:1800])
```

```{r fig.width=3.5, fig.height=3,echo=FALSE}
img <- readPNG("img/STIlinear.png")
grid.raster(img)
```

```{r}
plotNA.imputations(sample$open.STI[1000:1800], spline_imputed_data$open.STI[1000:1800])
```

```{r fig.width=3.5, fig.height=3,echo=FALSE}
img <- readPNG("img/STIspline.png")
grid.raster(img)
```

```{r}
plotNA.imputations(sample$open.STI[1000:1800], constant_imputed_data$open.STI[1000:1800])
```

Linear interploation looks a like a good way of dealing with missing data. 

## Working on what variables to use for predicting RVs

As RVs are generally consistent over time, using lagged values as a variable would be a good idea. Here are the autocorrelation charts for each of the RV series-

```{r}
par(mfrow=c(4,4))
col <- seq(from=52, to=94, by = 3)
for (j in col){
   acf(linear_imputed_data[,j], main=paste0(names(linear_imputed_data)[j]))
}
```

```{r fig.width=7, fig.height=4,echo=FALSE}
img <- readPNG("img/acf1.png")
grid.newpage()
grid.raster(img)
```

```{r fig.width=7, fig.height=4,echo=FALSE}
img <- readPNG("img/acf2.png")
grid.newpage()
grid.raster(img)
```

We would also be interested in seeing how the cross rv terms relate. As the interactions would be large (31C2 i.e. 465 terms), one of the ways that we decided to look at is average correlation over time using 500 values in each window. (Let us remove BVLG, FTMIB, STI and remove 1152 intial rows (now data starts from roughly Oct 2005) as their std would be zero when a constant value is imputed)


```{r}
col <- seq(from=4, to=94, by = 3)
rvs <- na_interpolation(grid[1512:5303,col], option = "linear") 
rvs <- rvs[,-c(5,9,30)]
covars <- c()
window <- 500
for (i in 1:(nrow(rvs)-(window-1))){
  sub <- rvs[i:(window-1+i),]
  mean_cor <- mean(cor(sub))
  covars <- c(covars, mean_cor)
}
plot(as.Date(grid$Date[2011:5303]), covars, type='l', xlab="Years", ylab="Average Correlation", main = "Rolling 500 days Correlation") # roughly starting from Sept, 2007
```

```{r fig.width=7, fig.height=5,echo=FALSE}
img <- readPNG("img/correlation.png")
grid.newpage()
grid.raster(img)
```

*90 day running correlation (linear)*

```{r}
col <- seq(from=4, to=94, by = 3)
rvs <- na_interpolation(grid[1512:5303,col], option = "linear") 
rvs <- rvs[,-c(5,9,30)]
covars <- c()
window <- 90
for (i in 1:(nrow(rvs)-(window-1))){
  sub <- rvs[i:(window-1+i),]
  mean_cor <- mean(cor(sub))
  covars <- c(covars, mean_cor)
}
plot(as.Date(grid$Date[1601:5303]), covars, type='l', xlab="Years", ylab="Average Correlation", main = "Rolling 90 days Correlation") # roughly starting from Sept, 2007
```


```{r}
col <- seq(from=4, to=94, by = 3)
rvs <- na.locf(grid[1512:5303,col], na.rm=FALSE) 
rvs <- rvs[,-c(5,9,30)]
covars <- c()
window <- 90
for (i in 1:(nrow(rvs)-(window-1))){
  sub <- rvs[i:(window-1+i),]
  mean_cor <- mean(cor(sub))
  covars <- c(covars, mean_cor)
}
plot(as.Date(grid$Date[1601:5303]), covars, type='l', xlab="Years", ylab="Average Correlation", main = "Rolling 90 days Correlation") # roughly starting from Sept, 2007
```

This variable could be used for switching between models as when market is in distress, correlations tend to peak and maybe a different model from usual could be suggested.

## Addtional datasets that we can use

Maybe like NVIX or Economic Uncertainity Data (It is however only mothly and last data point available is Jan 2020)

## Checking for normality of $\epsilon_t$

We calculate daily standardized returns and check for their normality using Shapiro-Wilk Normality Test. If p-value given by the test is greater than 0.05 then we can not reject the normality assumption.

```{r}
#grid
col <- seq(from=2, to=92, by = 3)
pvalsw <- c()
pvaljb <- c()
for (x in col){
  sub <- grid[,c(x,x+1,x+2)]
  eps <- log(sub[,1]/sub[,2])/sqrt(sub[,3])
  eps <- eps[!is.na(eps)]
  if (length(eps)> 5000){
    eps <- tail(eps, 5000)
  }
  a <- shapiro.test(eps)
  b <- jarque.bera.test(eps)
  pvalsw <- c(pvalsw, a$p.value)
  pvaljb <- c(pvaljb, b$p.value)
  #qqnorm(eps)
  #abline(0,1)
  
  #h <- hist(eps, breaks = 10, density = 10,
          #col = "lightgray", xlab = "Accuracy", main = "Overall") 
  #xfit <- seq(min(eps), max(eps), length = 40) 
  #yfit <- dnorm(xfit, mean = mean(eps), sd = sd(eps)) 
  #yfit <- yfit * diff(h$mids[1:2]) * length(eps) 
  #lines(xfit, yfit, col = "black", lwd = 2)
}

```

At 5% significance level, we have to reject the null hypothesis (that the data is normal) for 24 out of 31 indices.

```{r}
plot(pvalsw, main="P-values for different Indices - SW", ylab='p value')
abline(h=0.05, lty=2, col='red')
```

```{r fig.width=5, fig.height=4,echo=FALSE}
img <- readPNG("img/pvalue.png")
grid.newpage()
grid.raster(img)
```

```{r}
plot(pvaljb, main="P-values for different Indices - JB", ylab='p value')
abline(h=0.05, lty=2, col='red')
```

Plotting both p-values together
```{r}
plot(pvalsw, pvaljb)
abline(h=0.05, lty=2, col='red')
abline(v=0.05, lty=2, col='red')
```

So the two tests agree on all indices but one:
```{r}
print(which(pvalsw > 0.05))
print(which(pvaljb > 0.05))
```

# Building Baseline models

We want to start the forecasts from February 1st, 2020.

a) Random Walk: For predicting value at time t, use the value at time t-1.

```{r}
col <- c(1,seq(from=4, to=94, by = 3))
sample$Date <- as.Date(sample$Date)
testdata <- sample[year(sample$Date)==2020,]

#rv_actual has NAs, the idea would be to not make any predictions for those
rv_actual <- testdata[month(testdata$Date)>=2,col ]

#forward filling data so that we look at the most recent available value
rv_for_pred <- na.locf(na.locf(testdata[,col]), fromLast = TRUE)
rv_for_pred$Date <- shift(rv_for_pred$Date , -1)

#par(mfrow=c(3,3))
mse <- c()
for (s in 2:32){
  pick <- na.omit(rv_actual[,c(1,s)])
  pred <- merge(pick,rv_for_pred[,c(1,s)], by=c("Date"))
  #plot((pred[,2]- pred[,3])^2)
  mse <- c(mse, mean((pred[,2]- pred[,3])^2))
}

plot(mse, main="Mse for each index with Random Walk Predcitions")
```

```{r fig.width=5, fig.height=4,echo=FALSE}
img <- readPNG("img/randomMSE.png")
grid.newpage()
grid.raster(img)
```

b) Heterogeneous Autoregressive (HAR) model: We can use 200 days (to account for recency) rolling window for our predictions?

```{r}
col <- c(1,seq(from=4, to=94, by = 3))
traindata <- na.locf(na.locf(tail(sample[,col], 1000)), fromLast = TRUE)
mean5 <- as.data.frame(rollmean(zoo(traindata[, -c(1)]), 5))
mean22 <- as.data.frame(rollmean(zoo(traindata[, -c(1)]), 22))

# for 200 day rolling windows
rv <- tail(traindata, 239)
rv5 <- tail(mean5, 239)
rv22 <- tail(mean22, 239)

rv_actual <- testdata[month(testdata$X)>=2,col ]

#par(mfrow=c(3,3))
mse_har <- c()
for (s in 2:32){
  err <- c()
  for (x in 1:38){
    tab <- as.data.frame(cbind(rv[x:(199+x),s], rv5[x:(199+x),(s-1)], rv22[x:(199+x),(s-1)], rv[(x+1):(200+x),s]))
    linearMod <- lm(V4 ~ ., data=tab) 
    x_new <- as.data.frame(cbind(rv[(200+x),s], rv5[(200+x),(s-1)], rv22[(200+x),(s-1)]))
    rv_pred <- predict(linearMod, x_new)
    rv_actual_val <- rv_actual[x,s]
    # check error only when the actual value is present
    if (is.na(rv_actual_val) == FALSE){
      err <- c(err, (rv_pred - rv_actual_val)^2)
    }
  }
  #plot(err)
  mse_har <- c(mse_har, mean(err))
}
plot(mse_har, main="Mse for each index with HAR Predictions")

```

```{r fig.width=5, fig.height=4,echo=FALSE}
img <- readPNG("img/msehar.png")
grid.newpage()
grid.raster(img)
```

Comparing HAR mse (black) with Random Walk MSE (red) - in this case Random Walk does better.

```{r fig.width=5, fig.height=4,echo=FALSE}
img <- readPNG("img/mse2.png")
grid.newpage()
grid.raster(img)
```
