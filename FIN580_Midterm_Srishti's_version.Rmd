---
title: 'FIN580: Midterm'
author: "Chris, Utsav, Srishti"
date: "Spring 2020"
output:
  html_document:
    df_print: paged
    toc: no
    toc_depth: 4
  word_document:
    toc: no
    toc_depth: '4'
  pdf_document:
    fig_caption: yes
    toc: no
    toc_depth: 4
geometry: margin=1.5in
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.align="center", fig.height=5.5, fig.width=6, collapse=TRUE, comment="", prompt=TRUE, echo = TRUE, cache=TRUE, autodep=TRUE, cache.comments=FALSE)
options(width=63)
```

--------

# Objective

To forecast daily volatilities of major stock indexes after the COVID-19 outbreak and provide risk measures than can help portfolio managers to make investment decisions during this turbulent period.

--------

# Data Preparation

I use the following libraries and set my working directory.
```{r eval = F}
library('fasttime')
library('data.table')
library('dplyr')
library('matrixStats')
library('bit64')
library('ggplot2')
library('gdata')
library('naniar')
library('mice')
library('forecast')
library('imputeTS')
library('zoo')
library('tseries')
library('gtrendsR')
library('reshape2')
setwd("/Users/srishtisingla/Downloads")
```

Our dataset has values for daily open, close and RV for 31 indices. 

```{r eval = F}
data = fread('oxfordmanrealizedvolatilityindices.csv') 
data <- data[, c("V1", "Symbol", "open_price", "close_price", "rv5")]
names(data)[1] <- "Date"
summary(data)
```

```{r eval = F}
data = fread('data.csv') 
summary(data)
```

```{r eval = F}
times <- substr(data$Date, 12, nchar(data$Date))
unique_times <- unique(times)
dates1 <- unique(substr(data[substr(Date, 12, 26) == unique_times[1]]$Date, 1, 10))
dates2 <- unique(substr(data[substr(Date, 12, 26) == unique_times[2]]$Date, 1, 10))
intersect(dates1, dates2) # returns character(0), so can drop times
```

```{r eval = F}
symbols <- unlist(unique(data[, "Symbol"]))
for (sym in symbols) {
  subset_dates <- data["Symbol" == sym, "Date"]
  if (length(subset_dates) != length(unique(subset_dates))) {
    print(sym) 
  }
}
print("Done")
```

We first transform our data to show these values as columns (three for each index) such that each row corresponds to a unique day.

```{r eval = F}
#transforming data to get value for each security as a column
grid <- as.data.frame(as.character(unique(data$Date)))
names(grid) <- "Date"
symbols <- as.character(unique(data$Symbol))
for (i in 1:length(symbols)){
  mergewith <- data[as.character(data$Symbol)==symbols[i],-c(2)]
  names(mergewith) <- c("Date", paste0("open",symbols[i]), paste0("close",symbols[i]), paste0("rv",symbols[i]))
  grid <- merge(grid, mergewith, by=c("Date"), all = TRUE )
}
# grid has 5317 rows, and 94 columns

grid$Date <- as.Date(grid$Date)
sample <- grid[year(grid$Date)>=2010,]

# 2679 rows to get data starting from 2010 (we need to choose this carefully)

open_col_names <- colnames(grid[, seq(from=2, to=92, by = 3)])
index_names <- substr(open_col_names, 6, nchar(open_col_names))
```

```{r eval = F}
head(sample[, 1:7])
```

Checking how much data is missing-

```{r eval = F}
col <- c(1, seq(from=2, to=92, by = 3))
vis_miss(sample[,col]) # total 6.7% missing data, most for BVLG and STI
#gg_miss_var(sample[,col])
```

```{r fig.width=10, fig.height=7,echo=FALSE}
library(png)
library(grid)
img <- readPNG("missingdata.png")
 grid.raster(img)
```

```{r fig.width=5, fig.height=3.5,echo=FALSE}
library(png)
library(grid)
img <- readPNG("missingcount.png")
 grid.raster(img)
```

Also, we check what are the sizes of continuous gaps of data. The max continuous gaps as we already know are for BVLG and STI in the begining of the data. Apart from that the gap range is at max 5.

```{r eval = F}
# max continuousNA cases are in beginining if 
plotNA.gapsize(sample[,j])
```

## Different ways of dealing with missing data

**Linear interpolation**

```{r eval = F}
linear_imputed_data <- na_interpolation(sample, option = "linear") 
```

**Spline interpolation**

```{r eval = F}
spline_imputed_data <- na_interpolation(sample, option = "spline") 
```

**Constant imputation**

```{r eval = F}
constant_imputed_data <- na.locf(sample, na.rm=FALSE)
```
To visualize what's happening-

Want to see what happens in case of an index like AORD

```{r eval = F}
plotNA.imputations(sample$open.AORD[1:300], linear_imputed_data$open.AORD[1:300])
```

```{r fig.width=3.5, fig.height=3,echo=FALSE}
library(png)
library(grid)
img <- readPNG("AORDlinear.png")
 grid.raster(img)
```

```{r eval = F}
plotNA.imputations(sample$open.AORD[1:300], spline_imputed_data$open.AORD[1:300])
```

```{r fig.width=3.5, fig.height=3,echo=FALSE}
library(png)
library(grid)
img <- readPNG("AORDspline.png")
 grid.raster(img)
```

Constant Imputation

```{r eval = F}
plotNA.imputations(sample$open.AORD[1:300], constant_imputed_data$open.AORD[1:300])
```

For STI which has a lot of missing data in the beginning itself what do the two models do-

(The linear method basically fills one value for them all whereas spline does not)

```{r eval = F}
plotNA.imputations(sample$open.STI[1000:1800], linear_imputed_data$open.STI[1000:1800])
```

```{r fig.width=3.5, fig.height=3,echo=FALSE}
library(png)
library(grid)
img <- readPNG("STIlinear.png")
 grid.raster(img)
```

```{r eval = F}
plotNA.imputations(sample$open.STI[1000:1800], spline_imputed_data$open.STI[1000:1800])
```

```{r fig.width=3.5, fig.height=3,echo=FALSE}
library(png)
library(grid)
img <- readPNG("STIspline.png")
 grid.raster(img)
```

```{r eval= F}
plotNA.imputations(sample$open.STI[1000:1800], constant_imputed_data$open.STI[1000:1800])
```

Linear interploation looks a like a good way of dealing with missing data. 

## Working on what variables to use for predicting RVs

As RVs are generally consistent over time, using lagged values as a variable would be a good idea. Here are the autocorrelation charts for each of the RV series-

```{r eval = F}
par(mfrow=c(4,4))
col <- seq(from=52, to=94, by = 3)
for (j in col){
   acf(linear_imputed_data[,j], main=paste0(names(linear_imputed_data)[j]))
}
```

```{r fig.width=7, fig.height=4,echo=FALSE}
library(png)
library(grid)
img <- readPNG("acf1.png")
 grid.raster(img)
```

```{r fig.width=7, fig.height=4,echo=FALSE}
library(png)
library(grid)
img <- readPNG("acf2.png")
 grid.raster(img)
```

We would also be interested in seeing how the cross rv terms relate. As the interactions would be large (31C2 i.e. 465 terms), one of the ways that we decided to look at is average correlation over time using 500 values in each window. (Let us remove BVLG, FTMIB, STI and remove 1152 intial rows (now data starts from roughly Oct 2005) as their std would be zero when a constant value is imputed)


```{r eval = F}
par(mfrow=c(1,1))
sample <- grid[year(grid$Date)>=2019,]
col <- seq(from=4, to=94, by = 3)
rvs <- na.locf(sample[,col], na.rm=FALSE) 
#rvs <-  na_interpolation(sample[,col], option = "linear") 
#rvs <- rvs[,-c(5,9,30)]
window <- 90
covars <- c()
for (i in 1:(nrow(rvs)-(window-1))){
  sub <- rvs[i:(window-1+i),]
  mean_cor <- mean(cor(sub))
  covars <- c(covars, mean_cor)
}
plot(tail(grid$Date, length(covars)), covars, type='l', xlab="Years", ylab="Average Correlation", main = "Rolling 500 days Correlation", col='blue' )
```

```{r eval = F}
correl <- as.data.frame(cbind(tail(grid$Date, length(covars)), covars))
correl$V1 <- as.Date(correl$V1)
names(correl)[1] <- "Date"
correl <- na.omit(correl)
sample <- grid[((year(grid$Date)>=2019 & month(grid$Date)>= 7)) | (year(grid$Date)>=2020),]
correl <- correl[((year(correl$Date)>=2019 & month(correl$Date)>= 7)) | (year(correl$Date)>=2020),]
finaldata <- merge(sample, correl, by=c("Date"), all = TRUE )
finaldata <- na.locf(na.locf(finaldata, na.rm=FALSE) , na.rm = TRUE)
write.csv(finaldata,"final_data.csv")
```


```{r fig.width=7, fig.height=5,echo=FALSE}
library(png)
library(grid)
img <- readPNG("correlation.png")
 grid.raster(img)
```

This variable could be used for switching between models as when market is in distress, correlations tend to peak and maybe a different model from usual could be suggested.

## Addtional datasets that we can use

Maybe like NVIX or Economic Uncertainity Data (It is however only mothly and last data point available is Jan 2020)

Google trends-
```{r eval = F}
words <- c("coronavirus")
countries <- c("US", "IT", "CN", "IN", "CN", "MX", "SG", "CH", "ES", "GB", "SE", "DK", "JP", "PK", "KR", "HK", "DE", "AU", "BE", "NL", "PT", "BR", "FR", "FI")
#US, Italy, Canada, India, China, Mexico, Singapore, Switzerland, Spain, United Kingdom, Stockholm, Copenhagen, Japan, Pakistan, South Korea, Hong Kong, Germany, Australia, Belgium, Netherlands, Portugal, Brazil, France, Finland

start_date <- "2020-01-01"
end_date <- "2020-04-16"
toReplace <- 0.5 # numeric chosen to represent "<1" in data

datalist = list()
i <- 1
for(w_idx in seq(length(words))) {
  for(c_idx in seq(length(countries))) {
    google.trends <- gtrends(words[w_idx], geo = countries[c_idx], 
                            gprop = "web", time = paste(start_date, end_date))[[1]]
    google.trends <- dcast(google.trends, date ~ keyword + geo, value.var = "hits")
    rownames(google.trends) <- google.trends$date
    google.trends$date <- NULL
    datalist[[i]] <- google.trends
    i <- i + 1
  }
}

big_data <- do.call(cbind, datalist)
big_data[big_data == "<1"] <- toReplace
```

```{r eval = F}
par(mfrow=c(4,3))
for (i in 13:24){
  plot(d[,(i+1)], type = 'l', main=paste0(countries[i]), ylab='Word Frequency', xlab='Days')
}
```

```{r eval = F}
d <- cbind(rownames(big_data), data.frame(big_data, row.names=NULL))
names(d)[1] <- "Date"
big_data <- melt(d ,  id.vars = 'Date', variable.name = 'series')
#ggplot(big_data, aes(as.Date(Date),value)) + geom_line(aes(colour = series)) FIX THIS
```

## Checking for normality of $\epsilon_t$

We calculate daily standardized returns and check for their normality using Shapiro-Wilk Normality Test. If p-value given by the test is greater than 0.05 then we can not reject the normality assumption.

```{r eval = F}
#grid
col <- seq(from=2, to=92, by = 3)
pvalsw <- c()
pvaljb <- c()
for (x in col){
  sub <- finaldata[,c(x,x+1,x+2)]
  #sub <- grid[,c(x,x+1,x+2)]
  eps <- log(sub[,2]/sub[,1])/sqrt(sub[,3])
  eps <- eps[!is.na(eps)]
  if (length(eps)> 5000){
    eps <- tail(eps, 5000)
  }
  a <- shapiro.test(eps)
  b <- jarque.bera.test(eps)
  pvalsw <- c(pvalsw, a$p.value)
  pvaljb <- c(pvaljb, b$p.value)
  
  #qqnorm(eps)
  #abline(0,1)
  
  #h <- hist(eps, breaks = 10, density = 10,
          #col = "lightgray", xlab = "Accuracy", main = "Overall") 
  #xfit <- seq(min(eps), max(eps), length = 40) 
  #yfit <- dnorm(xfit, mean = mean(eps), sd = sd(eps)) 
  #yfit <- yfit * diff(h$mids[1:2]) * length(eps) 
  #lines(xfit, yfit, col = "black", lwd = 2)
}

```

At 5% significance level with S-W test, we have to reject the null hypothesis (that the data is normal) for 24 out of 31 indices.

```{r eval = F}
plot(pvalsw, main="P-values for different Indices - SW", ylab='p value')
abline(h=0.05, lty=2, col='red')
```

```{r eval = F}
plot(pvalsw, pvaljb, main="P-values for both tests - SW and JB")
abline(h=0.05, lty=2, col='red')
abline(v=0.05, lty=2, col='red')
```

```{r fig.width=5, fig.height=4,echo=FALSE}
library(png)
library(grid)
img <- readPNG("pvalue.png")
 grid.raster(img)
```

# Building Baseline models

We want to start the forecasts from February 1st, 2020.

a) Random Walk: For predicting value at time t, use the value at time t-1.

```{r eval = F}
col <- c(1,seq(from=4, to=94, by = 3))
testdata <- finaldata[year(finaldata$Date)==2020,col]

#Data starting Feb, 2020
rv_actual <- testdata[month(testdata$Date)>=2, ]

#shifting data
testdata$Date <- shift(testdata$Date , -1)

#par(mfrow=c(3,3))
mse <- c()
for (s in 2:32){
  pred <- merge(rv_actual[,c(1,s)],testdata[,c(1,s)], by=c("Date"))
  #plot((pred[,2]- pred[,3])^2)
  mse <- c(mse, mean((pred[,2]- pred[,3])^2))
}
plot(mse, main="Mse for each index with Random Walk Predcitions")
```

```{r fig.width=5, fig.height=4,echo=FALSE}
library(png)
library(grid)
img <- readPNG("randomMSE.png")
 grid.raster(img)
```

b) Heterogeneous Autoregressive (HAR) model: We can use 200 days (to account for recency) rolling window for our predictions?

```{r eval = F}
col <- c(1,seq(from=4, to=94, by = 3))
traindata <- finaldata[,col]
mean5 <- as.data.frame(rollmean(zoo(traindata[, -c(1)]), 5))
mean22 <- as.data.frame(rollmean(zoo(traindata[, -c(1)]), 22))

testdata <- finaldata[year(finaldata$Date)==2020,col]
rv_actual <- testdata[month(testdata$Date)>=2, ]
n <- nrow(rv_actual)

# for 90 day rolling windows
window <- 90
rv <- tail(traindata, (window+n+1))
rv5 <- tail(mean5, (window+n+1))
rv22 <- tail(mean22, (window+n+1))

#par(mfrow=c(3,3))
mse_har <- c()
for (s in 2:32){
  err <- c()
  for (x in 1:n){
    tab <- as.data.frame(cbind(rv[x:(window - 1 +x),s], rv5[x:(window - 1+x),(s-1)], rv22[x:(window - 1 +x),(s-1)], rv[(x+1):(window+x),s]))
    linearMod <- lm(V4 ~ ., data=tab) 
    x_new <- as.data.frame(cbind(rv[(window+x),s], rv5[(window+x),(s-1)], rv22[(window+x),(s-1)]))
    rv_pred <- predict(linearMod, x_new)
    rv_actual_val <- rv_actual[x,s]
    err <- c(err, (rv_pred - rv_actual_val)^2)
  }
  #plot(err)
  mse_har <- c(mse_har, mean(err))
}
plot(mse_har, main="Mse for each index with HAR Predictions")

```

```{r fig.width=5, fig.height=4,echo=FALSE}
library(png)
library(grid)
img <- readPNG("msehar.png")
 grid.raster(img)
```

Comparing HAR mse (black) with Random Walk MSE (red) - in this case Random Walk does better.

```{r fig.width=5, fig.height=4,echo=FALSE}
library(png)
library(grid)
img <- readPNG("mse2.png")
 grid.raster(img)
```
